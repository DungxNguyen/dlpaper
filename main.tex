\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{url}
\usepackage{pbox}
\usepackage{xcolor}
%\usepackage[colorlinks]{hyperref}
%\hypersetup{linkcolor=blue, citecolor=blue, urlcolor=blue}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\graphicspath{{./ImportedFigures/}}

\ifCLASSINFOpdf
\else
\fi
\hyphenation{op-tical net-works semi-conduc-tor}

\newif\ifdraft
\drafttrue
\ifdraft
\usepackage{xcolor}
\usepackage{array}
\usepackage{makecell}
\renewcommand\cellalign{cl}
\definecolor{ocolor}{rgb}{1,0,0.4}
\newcommand{\aanote}[1]{ {\textcolor{red} { ***amy: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andre: #1 }}}
\newcommand{\ednote}[1]{ {\textcolor{brown} { ***eddie: #1 }}}
\newcommand{\kknote}[1]{ {\textcolor{green} { ***ken: #1 }}}
\newcommand{\dungnote}[1]{ {\textcolor{orange} { ***dung: #1 }}}
\else
\newcommand{\aanote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\ednote}[1]{}
\newcommand{\kknote}[1]{}
\newcommand{\dungnote}[1]{}
\fi

\begin{document}

\title{Time-Constrained Processing of Deep Learning Computer Vision Applications in  Automotive Manufacturing}
% * <dungx.ngt@gmail.com> 2017-10-02T03:34:15.504Z:
%
% ^.


\maketitle

% These two lines add back the page numbers; we may nhttps://v2.overleaf.com/9174271949jhbybhjxjrsbeed to remove
% before submitting
\thispagestyle{plain}
\pagestyle{plain}



\begin{abstract}
In this paper, we investigate the trade-off between resource/cost consumption with running time and accuracy of different object detector models based on deep neural networks.
Execution is in an automotive manufacturing environment that places deadlines on the completion of the object detection application.  
\end{abstract}


\IEEEpeerreviewmaketitle

\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction and Motivating Application}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Deep learning system have become pervasive in the automotive domain. For example, an increasing number of visual inspection tasks in automotive manufacturing is conducted using camera-based systems and different kinds of classifier, detection and segmentation networks. A challenge is the data-intensiveness and computational complexity of these algorithms. One challenge is the deployment of this tools in the business process. As these algorithms are typically run on the edge device to ensure real-time feedback and the ability to react on the results in the business process. At the same time the usage of cloud is important to (i) utilize more complex deep learning models for higher accuracies and (ii) to combine data with other data, such as process and data from further sensors.


We aim to compare different families of object detectors in different devices: from state-of-the-art multiple GPUs workstations to resource-limited devices such as mini computers (Raspberry Pi, NIVDIA ...) and mobile devices (iPhone) or deep learning-specialized devices (Amazon DeepLens).

\alnote{Motivate streaming}

The difficulties of this project come from the differences of deep learning framework environments on different devices with different levels of optimization and efficiency, the wide varieties of object detectors, in both meta-architectures and feature extractors, and the constraints of the execution environment that imposes hard deadlines on the completion of the object detection methods.

For our applications, we only consider the test-time performance, which concerns about inference time, deployment time, memory footprint and hardware utilization in test-time only.

We illustrate a big picture of deep network object detectors for many different devices and purposes. Very light-weight devices such as Raspberry Pi require a small network and efficient run time environment to produce minimal memory footprint. Other devices requires real-time processing time to provide smooth user experience, particularly on smartphones. Different applications and devices have their own unique requirements and characteristics. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

Benchmarking deep learning system is investigated

\begin{itemize}
    \item MLPerf
    \item DeepBench
    \item Huang~\cite{DBLP:journals/corr/HuangRSZKFFWSG016} investigates the speed/accuracy trade-off f
\end{itemize}

In contrast to the approaches referenced above our benchmark is 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architecture Backgrounds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Computer vision application requires relatively small amount of pre-processing tasks in comparison with other fields. The only required pre-processing is to make sure all pixels are in the same scale. A typical pre-processing task is contrast normalization, which is proportional with L-2 normalization of "each" pic: subtracting the mean and divided by standard deviation. It can be done in global (the whole picture) or local. 

Contrast normalization (~L-2 normalization) makes all data points have the same (or similar) vector size (same norm). It makes learning more feasible because neural units react better with directions in space with different sets of coefs and worse with distance (bias). It is also beneficial to augment the dataset with different transformations of images (like rotating, cropping, adding noise) to have better generalization.

Computer vision exploits convolutional networks to produce state-of-the-art results in many applications, such as image classification or object detection.

Convolutional networks are neural networks that use a convolutional function between input and a kernel. The kernel works like a mask, iterates all over the spatial space of an input layer. Convolutional networks leverages 3 important ideas: sparse connections, parameter sharing and equivariant representations.

Usually, after each convolution layer, there is an activate layer (i.e reclu) and a pooling layer, which aggregate results from several nearby elements. The pooling layer helps increase the invariant of output to input.

Pooling is a special step in convolutional layer. It groups several elements and applies a function to them. Spatial function works with nearby elements in spatial space, while multi-channel pool groups elements from independently learned parameters (from different kernel).

Convolutional networks create very huge capacity networks, but constraints the weights to be mostly zeros (because kernels are relatively small in sizes) and equal to other (many parts of data input shares same kernel weights).

There are multiple degrees of freedom in object detection architectures:

\begin{itemize}
    \item Meta-Architecture: SSD, DeepMultiBox, R-FCN, Faster R-CNN, YOLO, ...
    \item Feature Extractor: VGG (multiple versions), Resnet (m), Inception (m), Inception Resnet, MobileNet. At least 6 are reported in~\cite{huang2017speed}. Feature Extractors are pre-trained on ImageNet-CLS (Image Classification task) first, and then are used to initialize the complete networks. 
    \item Feature Layers: Select which layer(s) of the feature extractor to be used in Meta-Architecture. Several papers select top (deepest) convolutional layer, but other papers encourage to aggregate multiple layers, even fully-connected layers.
    \item Box Proposals: Different original paper reports different number of proposals (10 .. 300 for an image of ~300x300 pixels)
    \item Trained Image size: 300 .. 600: R-FCN and Faster R-CNN are fixed at shorter edge. SSD is fixed at both edges.
    \item Stride (in feature extractor): reports show that stride is an important factor in the trade-off between performance (in mAP) and running time.
    \item Data Augmentation: Methods to transform images (rotation, cropping, etc) to make the models more robust.
\end{itemize}

Original papers usually just report 1 single combination of these options, with several variants such as different image resolutions, the numbers and positions of the candidate boxes, the layers from which features are extracted and the number of layers.

\dungnote{A strong argument against using original models but not combinations is the original models are optimized to get highest accuracy but not taking into account other trade-off such as memory or inference time. Several original models use different complex data augmentation methods. Another issue is original papers report results on different combinations of training sets (2007/2012 PASCAL VOC eval, 2007+2012 PASCAL VOC eval + test, etc}

\dungnote{A good thing is the paper at~\cite{huang2017speed} already produced a pipeline to re-train all the setup combinations. Therefore we only need to add/implement parts of new combinations. Source code can be found on Github. Training/Evaluation can be done only once in powerful workstations. We only need to record running time and resource utilization at edge devices.}

\dungnote{Pytorch doesn't have officially hosted trained models for object detections. They only have several image classification models in their official repository.}

\subsection{Meta-Architectures}

\alnote{good practical overview: \url{https://github.com/datadynamo/aiconf_ny_2018_pytorch_cv_tutorial/blob/master/AIConf_April2018_PyTorch_Computer_Vision_Part2.pdf}}

\alnote{Yolo v3?}
In general, SSD and YOLOv2 are used for fast, low-latency jobs while Faster R-CNN is often used for higher accuracy.

\subsubsection{DeepMultiBox}
DeepMultiBox is a box-generating model that becomes a basis for many other object detection architectures. Rather than a fully object detection model, it only predicts class-agnostic boxes with confident scores.

DeepMultiBox treats the class-agnostic bounding box as a regression problem of coordinates of boxes and their confident scores. The network predicts K boxes, with K is normally 100 or 200. Each box is described by 5 nodes in the last layer of the network, which are upper-left and bottom-right coordinates and a confident score in range $\{0, 1\}$.

To be faster and better in training, the ground truth boxes are clustered into K priors and the network will use the K priors as initilizations. Then, in the matching steps, rather than using the matches between predictions and ground truth boxes, the network uses the matches between these priors and ground truth ones. This process is called prior matching and is exploited by many other researches.

The original model uses the same neural architecture with AlexNet, only differs at the very top layer of the network. Loss function is calculated as weighted sum of localization errors and confident erros over all K prediction.


\subsubsection{YOLO and YOLO v2 and YOLO v3}
YOLO also uses single neural network without proposal generation process, combined with a customized feature extractor derived from GoogLeNet to predict classes and shapes of objects.


\subsubsection{SSD}
SSD is one of the fastest meta-architectures, along with variants of Yolo. SSD is more flexible~\alnote{what does flexible mean?} ~\dungnote{in Yolo, at least to version 2, you can not choose where you get the features, they are fixed in the network. In SSD you can choose whatever layers you want. By the way, the word ``flexible'' is used by the authors of SSD so I just think it's okay to reuse it}than YOLO because it uses prior boxes at different aspect ratio at different scale of feature map.~\cite{liu2016ssd} Because of this attribute, SSD can detect objects at different scales in images.

The original models are trained on PASCAL VOC (2007 and 2012 versions) and 2014 ILRSVRC datasets. The image's resolutions used in the original paper are $229 \times 229$ and $443 \times 443$.

SSD is short for Single Shot Detector, which means that the information flows through only 1 single neural network architecture in the model. It is contrast with other meta-achitectures such as R-CNN based models and R-FCN models, in which they contain at least a second convolutional structure to refine the results from a box proposal generator. Because of this characteristic, SSD is usually faster than R-CNN based models and R-FCN models in several orders of magnitude and consume less resource to run~\cite{huang2017speed}.

SSD does not have box proposal steps but calculate the bounding boxes and object's classes at the same time in a single propagation. The prior boxes are fixed at multiple scales for all images and the network will try to correct the priors to find the most prominent boxes.

From the optimization viewpoint, SSD calculate a single loss function for both bounding box localization and object classification errors using a weighted sum of these components.. 

Because of using prior boxes at different scales but not calculate the box proposals, SSD does not have good performance on small objects~\cite{liu2016ssd}.


\subsubsection{R-CNN }
R-CNN is a two-stage model to deal with object detection task: to generate bounded boxes and classify the object inside these boxes. The model is composed of three modules: Region Proposal, Feature Extraction and Region Classifications. Region Proposal module can be chosen from many methods already existed in literature, in which R-CNN chooses "selective search" to produce proposals. At inference time, the model produces 2000 proposals for each image. Feature Extraction transforms each region to a fix-sized feature vector by using the output of a hidden layer of a CNN model. Region Classification trains an SVM classifier, using fix-sized feature vectors produced by Feature Extraction.

Feature Extraction training is composed of two phases. The 1st phase is a supervised pre-training on an image classification task using a big dataset (where data are much more available). The 2nd phase replaces the output layer (because the incompatibility of the pre-training dataset and object detection dataset) then initializes the connection of last hidden layer with the new output layer and trains with box-bounded regions and their labels. A region's label is assigned to the ground-truth box's label for which they have more than 0.5 IoU overlap.

In the Region Classification, positive examples are ground-truth regions while negative examples are regions with less than 0.3 IoU overlap with the ground-truth ones.

R-CNN is originally pre-trained on PASCAL VOC 2007 and fine-tuned on PASCAL VOC 2012 datasets. 

\subsubsection{Faster R-CNN}
Faster R-CNN, which yields high accuracy object detection results, is a model that derived from R-CNN~\cite{girshick2015fast}.

\subsubsection{R-FCN}
R-FCN improves Faster R-CNN by delaying the cropping process to deeper layers of the net.

\subsubsection{SPPnet}
Improved version of R-CNN. This meta-architecture is out performed by Fast R-CNN and Faster R-CNN.

SPPnet enhances R-CNN by using a fixed-length feature maps, which are independent from the scale or resolution of images. In the last layers of feature networks, SPPNet applies spatial pyramid pooling layers to different regions of an image to create fixed-length feature maps.

The spatial pyramid pooling layers are placed in between convolutional layers and fully connected layers since convolutional layers can work with an abitrary size of input vectors, while the fully connected layers only work with fixed-length input vectors.

SPPnet has comparable accuracy with R-CNN but much faster in term of inference time.
\subsection{Feature Extractors}

\subsubsection{MobileNet}
MobileNet is an efficient deep net that is able to produce relatively light-weight networks but still have reasonable performance. ~\cite{howard2017mobilenets}

\subsubsection{VGG (Outdated)}

\subsubsection{Inception V3}

\subsubsection{Resnet}

\subsubsection{SqueezeNet}
This is a very light-weighted deep learning network~\cite{shen2018cs}

\subsubsection{XNOR Net}
This is a variant of binary weights network, operations in convolution layers are approximated by bit-wise operations (XNOR)~\cite{rastegari2016xnor}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Design and Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we describe the general approaches to resolve our motivating application and analyze the strength and weakness of the solutions.

*Decribe how a big picture is slpitted into small images*
\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{sample_image}
	  \caption{\textbf{Sample synthetic image used for  performance evaluation and profiling}}
	  \label{fig:sapmle_image}
\end{figure}

*THe tables that show the effective resolution of models*
\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{model_input_size}
	  \caption{\textbf{Effective Input Sizes of different models}}
	  \label{fig:model_input_size}
\end{figure}

*Reasoning why splitting into small images is a good tactic*

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Devices and Environment Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Devices}
In this section, we present the comparison between detailed specification devices

\subsubsection{NVIDIA V100}

\subsubsection{NVIDIA P100}

\subsubsection{NVIDIA TX2}

\subsection{Deep Learning Frameworks}
DeepX Toolkit is a framework used in resource efficiency deep learning optimization.

In this paper, we investigate 2 of common frameworks for deep learning applications: TensorFlow and Pytorch.

*Reasoning why we choose Tensorflow*

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Metrics and Profiling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Accuracy on ImageNet}
The common metric to measure accuracy in object detection is mAP, a common accuracy metric used in computer vision community.

The performance on ImageNet of several models in this paper are reported in original papers and other  researches~\cite{huang2017speed}.


\subsection{Deep Learning Performance}

MLPerf, \url{https://mlperf.org/}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Inference Time and Accuracy}

\subsection{Memory Consumption and Accuracy}

\subsection{Inference Time and Memory Consumption}

\subsection{Inference Time and Accuracy with restricted Memory Consumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Discussion about trade-off in streaming environment is analyzed and discussed in this section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~\cite{huang2017speed}~do a similar research, but their experiments are limited to powerful workstations, in which the implementations of the models are developed using Tensorflow framework. The accuracy of this paper's models are evaluated on COCO for object detection.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
%\setstretch{1}
%\setlength\bibitemsep{0pt}
\bibliography{deeplearning}




% that's all folks
\end{document}

