\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{url}
\usepackage{pbox}
\usepackage{xcolor}
%\usepackage[colorlinks]{hyperref}
%\hypersetup{linkcolor=blue, citecolor=blue, urlcolor=blue}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\graphicspath{{./ImportedFigures/}}

\ifCLASSINFOpdf
\else
\fi
\hyphenation{op-tical net-works semi-conduc-tor}

\newif\ifdraft
\drafttrue
\ifdraft
\usepackage{xcolor}
\usepackage{array}
\usepackage{makecell}
\renewcommand\cellalign{cl}
\definecolor{ocolor}{rgb}{1,0,0.4}
\newcommand{\aanote}[1]{ {\textcolor{red} { ***amy: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andre: #1 }}}
\newcommand{\ednote}[1]{ {\textcolor{brown} { ***eddie: #1 }}}
\newcommand{\kknote}[1]{ {\textcolor{green} { ***ken: #1 }}}
\newcommand{\dungnote}[1]{ {\textcolor{orange} { ***dung: #1 }}}
\else
\newcommand{\aanote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\ednote}[1]{}
\newcommand{\kknote}[1]{}
\newcommand{\dungnote}[1]{}
\fi

\begin{document}

\title{Edge Processing of Deep Learning Computer Vision Applications in  Automotive Manufacturing}


\maketitle

% These two lines add back the page numbers; we may nhttps://v2.overleaf.com/9174271949jhbybhjxjrsbeed to remove
% before submitting
\thispagestyle{plain}
\pagestyle{plain}



\begin{abstract}
Deep learning models are associated with various deployment challenges: they are typically very compute-intensive and memory-intensive.
Also, execution in our automotive manufacturing environment requires the input of a very large set of high definition images, and at the same time places runtime deadlines and accuracy requirements on the deep learning-based object detection of the images.
Meeting all of the user requirements of runtime, accuracy, and resource constraints by a deep learning object detection application is not possible without careful consideration of the choice of model, model parameters, hardware, and environmental support.
In this paper, we investigate the trade-offs of eighteen popular deep  neural network-based  object detection models on three hardware platforms.  
We report the trade-offs of resource consumption, runtime, and accuracy for a realistic automotive manufacturing environment.
\end{abstract}


\IEEEpeerreviewmaketitle

\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction and Motivating Application}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Deep learning system have become pervasive in the automotive domain. For example, an increasing number of visual inspection tasks in automotive manufacturing is conducted using camera-based systems and different kinds of classifier, detection and segmentation networks. A challenge is the data-intensiveness and computational complexity of these algorithms. One challenge is the deployment of this tools in the business process. As these algorithms are typically run on the edge device to ensure real-time feedback and the ability to react on the results in the business process. At the same time the usage of cloud is important to (i) utilize more complex deep learning models for higher accuracies and (ii) to combine data with other data, such as process and data from further sensors.


We aim to compare different families of object detectors in different devices: from state-of-the-art multiple GPUs workstations to resource-limited devices such as mini computers (Raspberry Pi, NIVDIA ...) and mobile devices (iPhone) or deep learning-specialized devices (Amazon DeepLens).

% Minicomputer -> embedded
% Devices 

\alnote{Motivate streaming}

The difficulties of this project come from the differences of deep learning framework environments on different devices with different levels of optimization and efficiency, the wide varieties of object detectors, in both meta-architectures and feature extractors, and the constraints of the execution environment that imposes hard deadlines on the completion of the object detection methods.

For our applications, we only consider the test-time performance, which concerns about inference time, deployment time, memory footprint and hardware utilization in test-time only.

We illustrate a big picture of deep network object detectors for many different devices and purposes. Very light-weight devices such as Raspberry Pi require a small network and efficient run time environment to produce minimal memory footprint. Other devices requires real-time processing time to provide smooth user experience, particularly on smartphones. Different applications and devices have their own unique requirements and characteristics. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architecture Backgrounds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Traditionally, Computer Vision applications used to be implemented by training machine learning models to low-level features such as SIFT. It requires complex feature engineering tasks to produce effective feature sets for different kinds of application. Nowadays, deep learning techniques are widely adopted due to their simplicity in image processing: They can be applied directly to raw images without complex feature extraction algorithms. Many common tasks such as image classification and object detection can be solved effectively using out-of-the-box deep learning architectures.

Among deep learning applications, Computer vision applications requires relatively small amount of pre-processing tasks in comparison with other fields such as natural language processing or speech recognition. The only required pre-processing is to make sure all pixels are in the same scale. A typical pre-processing task is contrast normalization, which is proportional with L-2 normalization of "each" pic: subtracting the mean and divided by standard deviation. It can be done in global (the whole picture) or local. 

Contrast normalization (~L-2 normalization) makes all data points have the same (or similar) vector size (same norm). It makes learning more feasible because neural units react better with directions in space with different sets of coefs and worse with distance (bias). It is also beneficial to augment the dataset with different transformations of images (like rotating, cropping, adding noise) to have better generalization.

Computer vision exploits convolutional networks to produce state-of-the-art results in many applications, such as image classification or object detection.

Convolutional networks are neural networks that use a convolutional function between input and a kernel. The kernel works like a mask, iterates all over the spatial space of an input layer. Convolutional networks leverages 3 important ideas: sparse connections, parameter sharing and equivariant representations.

Usually, after each convolution layer, there is an activate layer (i.e reclu) and a pooling layer, which aggregate results from several nearby elements. The pooling layer helps increase the invariant of output to input.

Pooling is a special step in convolutional layer. It groups several elements and applies a function to them. Spatial function works with nearby elements in spatial space, while multi-channel pool groups elements from independently learned parameters (from different kernel).

Convolutional networks create very huge capacity networks, but constraints the weights to be mostly zeros (because kernels are relatively small in sizes) and equal to other (many parts of data input shares same kernel weights).

\begin{figure*}[htpb]
	  \centering
	  \includegraphics[width=\textwidth]{original_models}
	  \caption{\textbf{Original models with their attributes.}}
	  \label{fig:original_models}
\end{figure*}

Object detection is a popular task that convolutional networks can solve effectively. Many different architectures are proposed in recent years, which have different characteristics. In object detection, a computer vision algorithm try to detect different objects in an input image and draw a rectangle box to localize the bounding area of eachobject. In some perspectives, object detection is consisted of two different tasks: object classification and bounding box regression. An object with its correspondent box is consider correct if (1) it is correctly classified and (2) its bounding box exceeds some levels of overlapping with manually labelled data, usually at least 50\%. Some models are designed to achieve state-of-the-art performance in accuracy. Several models aim to get reasonable accuracy within limited time and computational resources. There are efforts to build deep learning models for special hardware systems (FPGA) or resource-limited devices such as smartphones. There are multiple degrees of freedom in object detection architectures which affect their accuracy, running time and required resources:

\begin{itemize}
    \item Meta-Architecture: SSD, DeepMultiBox, R-FCN, Faster R-CNN, YOLO, ... which is categorized into single stage and 2-stage classes.
    \item Feature Extractor: VGG (multiple versions), Resnet (m), Inception (m), Inception Resnet, MobileNet. At least 6 are reported in~\cite{huang2017speed}. Feature Extractors are usually image classification networks which are pre-trained on common dataset such as ImageNet-CLS (Image Classification dataset) first, and then are used to initialize the complete networks. 
    \item Feature Layers: Select which layer(s) of the feature extractor to be used in Meta-Architecture. Several papers select the top (deepest) convolutional layer, but other papers encourage to aggregate multiple layers, even fully-connected layers.
    \item Box Proposals: Different original paper reports different number of proposals, as well as how to select proposals (10 .. 300 for an image of ~300x300 pixels)
    \item Trained Image size: 300 .. 600: R-FCN and Faster R-CNN are fixed at shorter edge. SSD is fixed at both edges.
    \item Stride (in feature extractor): reports show that stride is an important factor in the trade-off between performance (in mAP) and running time.
    \item Data Augmentation: Methods to transform images (rotation, cropping, etc) to make the models more robust.
\end{itemize}

Original papers usually just report 1 single combination of these options, with several variants such as different image resolutions, the numbers and positions of the candidate boxes, the layers from which features are extracted and the number of layers.

\dungnote{A strong argument against using original models but not combinations is the original models are optimized to get highest accuracy but not taking into account other trade-off such as memory or inference time. Several original models use different complex data augmentation methods. Another issue is original papers report results on different combinations of training sets (2007/2012 PASCAL VOC eval, 2007+2012 PASCAL VOC eval + test, etc}

\dungnote{A good thing is the paper at~\cite{huang2017speed} already produced a pipeline to re-train all the setup combinations. Therefore we only need to add/implement parts of new combinations. Source code can be found on Github. Training/Evaluation can be done only once in powerful workstations. We only need to record running time and resource utilization at edge devices.}

\dungnote{Pytorch doesn't have officially hosted trained models for object detections. They only have several image classification models in their official repository.}

\subsection{Meta-Architectures}

% \alnote{good practical overview: \url{https://github.com/datadynamo/aiconf_ny_2018_pytorch_cv_tutorial/blob/master/AIConf_April2018_PyTorch_Computer_Vision_Part2.pdf}}

% \alnote{Yolo v3?}
Meta-architectures in object detection models can be categorized into 2 different classes: single stage and 2-stage. In 2-stage models, images are passed through a first parts to produce box proposals (which are rectangle parts of the input images) and then the proposals are feed into the second stage to predict objects and re-calulate the bounding boxes. In single stage models, input images are passed through the networks once to produce predicted objects and their bounding boxes.

In general, single stage meta-architectures such as SSD and YOLOv2 are used for fast, low-latency jobs while 2-stage meta-architectures as Faster R-CNN and R-FCN are often used for better accuracy.

\subsubsection{DeepMultiBox}
DeepMultiBox is a box-generating model that becomes a basis for many other object detection architectures. Rather than a fully object detection model, it only predicts class-agnostic boxes with confident scores.

DeepMultiBox treats the class-agnostic bounding box as a regression problem of coordinates of boxes and their confident scores. The network predicts K boxes, with K is normally 100 or 200. Each box is described by 5 nodes in the last layer of the network, which are upper-left and bottom-right coordinates and a confident score in range $\{0, 1\}$.

To be faster and better in training, the ground truth boxes are clustered into K priors and the network will use the K priors as initilizations. Then, in the matching steps, rather than using the matches between predictions and ground truth boxes, the network uses the matches between these priors and ground truth ones. This process is called prior matching and is exploited by many other researches.

The original model uses the same neural architecture with AlexNet, only differs at the very top layer of the network. Loss function is calculated as weighted sum of localization errors and confident erros over all K prediction.


\subsubsection{YOLO and its variants}
YOLO also uses single neural network without proposal generation process, combined with a customized feature extractor derived from GoogLeNet to predict classes and shapes of objects.


\subsubsection{SSD}
SSD is one of the fastest meta-architectures, along with variants of Yolo. SSD is more flexible~\alnote{what does flexible mean?} ~\dungnote{in Yolo, at least to version 2, you can not choose where you get the features, they are fixed in the network. In SSD you can choose whatever layers you want. By the way, the word ``flexible'' is used by the authors of SSD so I just think it's okay to reuse it}than YOLO because it uses prior boxes at different aspect ratio at different scale of feature map.~\cite{liu2016ssd} Because of this attribute, SSD can detect objects at different scales in images.

The original models are trained on PASCAL VOC (2007 and 2012 versions) and 2014 ILRSVRC datasets. The image's resolutions used in the original paper are $229 \times 229$ and $443 \times 443$.

SSD is short for Single Shot Detector, which means that the information flows through only 1 single neural network architecture in the model. It is contrast with other meta-achitectures such as R-CNN based models and R-FCN models, in which they contain at least a second convolutional structure to refine the results from a box proposal generator. Because of this characteristic, SSD is usually faster than R-CNN based models and R-FCN models in several orders of magnitude and consume less resource to run~\cite{huang2017speed}.

SSD does not have box proposal steps but calculate the bounding boxes and object's classes at the same time in a single propagation. The prior boxes (or anchors) are fixed at multiple scales for all images and the network will try to correct the priors to find the most prominent box for each object.

From the optimization viewpoint, SSD calculate a single loss function for both bounding box localization and object classification errors using a weighted sum of 2 loss functions. 

Because of using prior boxes at different scales but not calculate the box proposals, SSD does not have good performance on small objects~\cite{liu2016ssd}.


\subsubsection{R-CNN }
R-CNN is a two-stage model to deal with object detection task: to generate bounded boxes and classify the object inside these boxes. The model is composed of three modules: Region Proposal, Feature Extraction and Region Classifications. Region Proposal module can be chosen from many methods already existed in literature, in which R-CNN chooses "selective search" to produce proposals. At inference time, the model produces 2000 proposals for each image. Feature Extraction transforms each region to a fix-sized feature vector by using the output of a hidden layer of a CNN model. Region Classification trains an SVM classifier, using fix-sized feature vectors produced by Feature Extraction.

Feature Extraction training is composed of two phases. The 1st phase is a supervised pre-training on an image classification task using a big dataset (where data are much more available). The 2nd phase replaces the output layer (because the incompatibility of the pre-training dataset and object detection dataset) then initializes the connection of last hidden layer with the new output layer and trains with box-bounded regions and their labels. A region's label is assigned to the ground-truth box's label for which they have more than 0.5 IoU overlap.

In the Region Classification, positive examples are ground-truth regions while negative examples are regions with less than 0.3 IoU overlap with the ground-truth ones.

R-CNN is originally pre-trained on PASCAL VOC 2007 and fine-tuned on PASCAL VOC 2012 datasets. 

\subsubsection{SPPnet}
SPPNet is an improved version of R-CNN to exploit the features produced by R-CNN convolutional network. This meta-architecture is faster than R-CNN but is out performed by Fast R-CNN and Faster R-CNN.

SPPnet enhances R-CNN by using a fixed-length feature maps, which are independent from the scale or resolution of images. In the last layers of feature networks, SPPNet applies spatial pyramid pooling layers to different regions of an image to create fixed-length feature maps.

The spatial pyramid pooling layers are placed in between convolutional layers and fully connected layers since convolutional layers can work with an abitrary size of input vectors, while the fully connected layers only work with fixed-length input vectors.

SPPnet has comparable accuracy with R-CNN but is much faster in term of inference time. Though being out-performed by other meta-architectures, SPPNet defines a new method to exploit feature vectors produced by convolutional networks. Fixed-length multi-layer feature vectors can capture highly sophisticated information from different scales and also works well with images of different resolutions.

\subsubsection{Fast R-CNN}
Fast R-CNN improves R-CNN and SPP Net by eliminating multi-stage learning. Fast R-CNN uses only one loss function for multi tasks (box creation, object detection). This architecture does not require an additional classifier such as SVM in R-CNN to classify boxes into categories. The classification task is embedded directly into the neural network structure of the model.

Fast R-CNN is an improvement architecture based on R-CNN and SPP Net. It uses the fixed-length feature vector from SPP Net to create more efficient features in feature extraction phase. Different from R-CNN, where each box proposal is passed through the network separately, Fast R-CNN passes the whole image through its convolutional network structure (VGG16) once and creates multiple ROI (Region of Interest) layers from multiple box proposals. Each ROI layer is associated with one proposal. The feature vectors extracted from ROI layers are passed through another neural network structure to calculate a single target value (of the loss function) for each box proposal.

Fast R-CNN also combines two tasks of object detection (object classification and box regression) into a single loss function. The unified loss function in Fast R-CNN is a weighted sum of a loss function of object classification error and another loss function of box regression error.

Another improvement of Fast R-CNN over R-CNN is using a softmax classifier rather than an SVM model in the final stage of classification. The softmax layers not only improves the accuracy of the model by a small margin but also combines two separated stages of the detector (a conv net and an svm model) in R-CNN into a single neural network structure. It makes the learning phase faster and more efficient, as well as a more straightforward prediction phase. 

Fast R-CNN still uses an external box proposals creation like R-CNN, which depends on an existed algorithm to create candidates for boxes. 

\subsubsection{Faster R-CNN}
Faster R-CNN, which yields high accuracy object detection results, is a model that derived from R-CNN~\cite{girshick2015fast}. Faster R-CNN is currently state-of-the-art meta-architecture in term of accuracy in object detection. Faster R-CNN eliminates the major drawback of Fast R-CNN, which is the dependency of Fast R-CNN on an external box proposals creator. By embedding the box proposal process into the neural network structure, Faster R-CNN increases both efficiency and accuracy of the previous models based on R-CNN.

Using a set of pre-defined anchor boxes as references, Faster R-CNN uses a part of the neural network structure to calculate the candidate box bounds and calls it Regional Proposal Network. This RPN shares components with the object detection network (Fast R-CNN) to utilizes calculated features from images. RPN and object detection network are trained separatedly. In inference time, box proposals are produced by RPN first, and then are passed through object detection network latter to form a 2-stage end-to-end object detector. Faster R-CNN is about an order of magnitude faster than Fast R-CNN while achieve better results on VOC dataset.

In training time, Faster R-CNN switches between region proposal optimization and object detection optimization. Because both tasks share a portion of the convolutional network, training time converges quickly. In inference time, the image is passed through the shared parts of the network first to produce (1) proposals that may contain objects and (2) feature maps used to predict objects and regress their bounding boxes. After getting results from proposal network, candidates go through another neural network structure to calculate prediction for objects and bounding boxes. The process above means Faster R-CNN is a two-stage detector, which requires 2 phases of computation and 2 seperated neural network sub-structures. 

Compared to Fast R-CNN, Faster R-CNN combines box proposal and object detection into a unified neural network structure, though they are still consisted of 2 phases. It eliminates the use of external algorithms to produce candidate boxes in Fast R-CNN and SPPNet which makes the object detection process faster and more efficient. Faster R-CNN is still state-of-the-art meta architecture at the time this paper is written.

\subsubsection{R-FCN}
R-FCN alters Faster R-CNN by delaying the cropping process in object detection phase to deeper layers of the network. While Faster R-CNN crops feature vectors directly from feature extractor network, R-FCN keeps passing the output of feature extractor network via another convolutional network structure and only crops its features right before the last layer of its network structure. This approach increases the share neurals between different croppings, which reduces time and computational resources.

Using the same feature extractor with Faster R-CNN, R-FCN produces comparable accuracy in several common datasets and is up to 20 times faster than Faster R-CNN.

\subsection{Feature Extractors}
Feature Extractors in object detection models are modified structures of image classification neural networks. Instead of producing prediction about classes of images, the network structures are produced only hidden layers. These hidden layers contain abstract information extracted from raw images. Depending on different configurations, an object detection model can select output of different hidden layers of Feature Extractors to feed further layers of its architecture. Common layers are selected to get data from are usually top-most hidden layer or several top hidden layers. These selected layers can be Relu layers and/or Pooling layers and/or fully connected layers. 

There are no clear evidence to interpret those hidden layers of neural networks. The selections of feature layers are usually based on heuristics and experiments. 

Normally, Feature Extractors are trained as fully functional image classification models on large datasets. It is another heuristics that if the image classification models can classify images with high accuracy, their hidden layers should capture useful visual information and they may work well with other computer vision tasks. By illustrating these hidden layers, it can be seen than different neural units response differently with particular objects and shapes, which is an evidence that these hidden layers contain abstract information about visual objects.


\subsubsection{VGG}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{vgg16}
	  \caption{\textbf{VGG16 architecture}. Need source.}
	  \label{fig:vgg16}
\end{figure}
VGG is a popular family of convolutional neural network first developed in 2014. A VGG network is a stack of multiple convolutional layers with max pooling layers, topped by 3 fully connected layer before producing outputs to a final softmax. VGG uses mask windows of size 1x1 and 3x3 in its convolutional layers, and places 2-4 convolutional layers between 2 consecutive max pooling layers. Original variants of VGG are consisted of VGG16 and VGG19, which have in total 16 and 19 weight layers respectively, in which they differ in the number of convolutional layers. VGG was a very deep model and one of the top model in image classification in 2014, then quickly become outdated.

VGG is considered large in term of number of trainable weights and very slow to train. Though it is now out-performed by more efficient architectures, VGG is still used as a baseline model because of its popularity and simplicity. Pre-trained VGGs are currently bundled with common deep learning frameworks. \dungnote{need citations}.

\subsubsection{Resnet}
Instead of building networks based on straightforward neural units, ResNet uses building blocks to construct its architecture. Each building block is composed of several neural network units, and has its own structure, which is called micro-architecture. Several micro-architectures are proposed, in which the most important one is residual block. In that block, residual component (which is identical to input) is forwarded to a deeper layer of the micro-architecture and is added to the result of the deeper layer to form a new value. Residual blocks also utilize batch normalization, a technique that normalize activations of a hidden layers to have zero mean and unit variance. Using these blocks, ResNet creates a very deep structure of neural network. Popular variants of ResNet contain 50, 101 or 200 weight layers.

Though ResNet is much deeper than VGG in term of number of layers, many weights inside ResNet are constrained or eliminated by not using fully connected layers as VGG, making the architecture more robust and more efficient.

\subsubsection{Inception V3}
Inception variants are also built using micro-architecture blocks as ResNet, but using a different structure. Differ from VGG where all convolutional units in the same layer have the same mask window size, convolutional units within the same layer in Inception may have different mask windown sizes. Moreover, these convolutional units may be stacked in a different ways to produce visual information in multiple scales and combination of scales.  

\subsubsection{SqueezeNet}
This is a very light-weighted deep learning network~\cite{shen2018cs} which has a complex training method to produce a relatively small network in term of weights (two orders of magnitude smaller than VGG) but still achieve comparable results in image classification.

\subsubsection{MobileNet}
MobileNet is an efficient deep net that is able to produce relatively light-weight networks but still have reasonable performance. ~\cite{howard2017mobilenets}. MobileNet utilizes 1x1 conv layer and a special type of convolutional layer called depthwise convolution. Depthwise convolution is a grouped convolution without convolution in channel domain.

\subsubsection{XNOR Net}
This is a variant of binary weights network, operations in convolution layers are approximated by bit-wise operations (XNOR)~\cite{rastegari2016xnor}.

\subsubsection{NAS }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Design and Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Complete automated inspection of vehicles based on computer vision imposes requirement on the data collection setup to provide high resolution images that would allow inference of large amount of information. In this report we focus on object detection only, however, the same images are used for measurements of the geometry of the car and finding any surface anomalies. To achieve the high accuracy inspection, the mapping of a single pixel on the image should not exceed 0.1 mm of the inspected area. An average height of the car on the assembly line being approximately 1500 mm needs to be therefore covered by 15000 pixels. This may be achieved by either using a single high resolution camera or multiple lower resolution cameras with the whole height of the car split between several images. Our application benefits from the later solution for several reasons: cost effectiveness of the several smaller resolution cameras over a single high resolution (at the 15000 pixel level), better control over lens distortion at small distance to an object, higher frame rate of cameras with smaller resolution. In addition to these, multi-camera setup allow us to have much better insight into depth information for inspection of the geometry. 

In this section, we describe the general approaches to resolve our motivating application and analyze the strength and weakness of the solutions.

*Decribe how a big picture is slpitted into small images*
\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{sample_image}
	  \caption{\textbf{Sample synthetic image used for  performance evaluation and profiling}}
	  \label{fig:sapmle_image}
\end{figure}

*THe tables that show the effective resolution of models*
\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{model_input_size}
	  \caption{\textbf{Effective Input Sizes of different models}}
	  \label{fig:model_input_size}
\end{figure}

*Reasoning why splitting into small images is a good tactic*

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Devices and Environment Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Deep Learning Frameworks}
DeepX Toolkit is a framework used in resource efficiency deep learning optimization.

In this paper, we investigate 2 of common frameworks for deep learning applications: TensorFlow and Pytorch.

*Reasoning why we choose Tensorflow*

\subsection{Devices}
In this section, we present the comparison between detailed specification devices

\subsubsection{NVIDIA V100}

\subsubsection{NVIDIA P100}

\subsubsection{NVIDIA TX2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Metrics and Profiling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Accuracy on ImageNet}
The common metric to measure accuracy in object detection is mAP, a common accuracy metric used in computer vision community.

The original performances on ImageNet of several models in this paper are reported in original papers and other researches~\cite{huang2017speed}. Accuracy performances of pre-trained models referred in this paper are reported in Object Detection API's Github repository. We only report these values because due to the differences in configurations, number of iterations or datasets, the accuracies between pre-trained models provided by Tensorflow's Object Detection API may not be consistent with original works.

\subsection{Inference Time and Memory Consumption}
In this section, we describe how we define and calculate the inference time and memory consumption of experimental models.

\subsubsection{Inference Time}
We calculate the average inference time per 1 test image ($2500 \times 2500$ pixels). Our inference time only includes splitting a test image into multiple tiles and making predictions from all tiles. Inference time does not include loading images from persistent storage devices, decoding images (for example, JPEG, if necessary), transforming images into Numpy tensors. Inference time ends when the results of object detection are calculated as output of detection graphs, in term of lists of tensors.

\subsubsection{GPU Memory Consumption}
In the default configuration, Tensorflow consumes all available GPU memory resources, which mean the memory utilization would be nearly $100\%$ during all run-times. Therefore, we examine the memory consumption with Tensorflow configuration $Growth = True$ in order to let the framework to start with minimum memory and to grow its memory when necessary. 

The memory consumption then is reported to each model as the maximum amount of allocated memory during each experiment. We sample the allocated memory values with an interval of 0.01 second.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results and Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% How separated parts ...

In this section, we discuss the experimental results and analyze these results in multiple perspectives and criterias.

\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllllll}
1 GPU V100Python 3.6, Tensorflow 1.7                            &               &                     &                    &                   &                   &                  &        &                   &             &          \\
Model name                                                      &               & Minibatch size: 108 & Minibatch size: 64 & Minibatch size 32 & Minibatch size 16 & Minibatch size 8 & 4      & Minibatch size: 2 & 1           & Accuracy \\
                                                                & Time II (sec) & 3.709               & 3.742              & 3.777             & 4.029             & 3.875            & 4.047  & 4.301             & 4.698       &          \\
ssd\_mobilenet\_v1\_coco                                        & Time (sec)    & 3.788               & 3.601              & 3.669             & 3.793             & 3.771            & 3.917  & 4.109             & 4.334       &          \\
                                                                & Memory(MiB)   & 2790                & 2790               & 1768              & 1254              & 998              & 870    & 808               & 806         &          \\
                                                                &               & 1.317               & 1.317              & 1.28              & 1.38              & 1.381            & 1.532  & 1.867             & 2.367       &          \\
ssd\_mobilenet\_v2\_coco                                        & Time (sec)    & 1.162               & 1.19               & 1.277             & 1.221             & 1.32             & 1.408  & 1.649             & 2.24        &          \\
                                                                & Memory (MiB)  & 4854                & 4854               & 3830              & 3318              & 3062             & 3062   & 2934              & 2934        &          \\
                                                                &               & 1.287               & 1.255              & 1.302             & 1.311             & 1.34             & 1.483  & 1.756             & 2.099       &          \\
ssdlite\_mobilenet\_v2\_coco                                    & Time (sec)    & 1.177               & 1.213              & 1.198             & 1.18              & 1.272            & 1.307  & 1.624             & 2.101       &          \\
                                                                & Memory(MiB)   & 2790                & 2822               & 1766              & 1254              & 998              & 870    & 806               & 806         &          \\
                                                                &               & 3.864               & 3.974              & 4.01              & 4.11              & 4.21             & 4.39   & 4.725             & 5.385       &          \\
ssd\_inception\_v2\_coco                                        & Time (sec)    & 3.67                & 3.887              & 3.889             & 3.901             & 4.071            & 4.066  & 4.579             & 5.277       &          \\
                                                                & Memory(MiB)   & 8934                & 8934               & 4838              & 3814              & 3304             & 3046   & 3046              & 3046        &          \\
                                                                &               & OOM                 & 2.033              & 2.115             & 2.133             & 2.26             & 2.429  & 2.848             & 3.78        &          \\
faster\_rcnn\_inception\_v2\_coco                               & Time (sec)    & OOM                 & 1.985/2.123        & 1.99/2.155        & 1.933             & 2.116            & 2.33   & 2.662             & 3.417       &          \\
                                                                & Memory(MiB)   &                     & 15610              & 8934              & 4842              & 4842             & 4842   & 4330              & 4074        &          \\
                                                                &               & OOM                 & 3.154              & 3.147             & 3.102             & 3.296            & 3.508  & 3.966             & 4.717       &          \\
faster\_rcnn\_resnet50\_coco                                    & Time (sec)    & OOM                 & 3.005/3.158        & 3.004/3.182       & 2.968/3.073       & 3.154            & 3.415  & 3.748             & 4.581       &          \\
                                                                & Memory(MiB)   &                     & 15610              & 8938              & 8934              & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & 2.237               & 2.253              & 2.183             & 2.232             & 2.398            & 2.644  & 3.024             & 3.683       &          \\
faster\_rcnn\_resnet50\_lowproposals\_coco                      & Time (sec)    & 2.152/2.243         & 2.164              & 2.164             & 2.179             & 2.244            & 2.491  & 2.799             & 3.561       &          \\
                                                                & Memory(MiB)   & 10986               & 15610              & 8938              & 8934              & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & 3.092               & 3.026              & 3.024             & 3.057             & 3.41             & 3.797  & 4.277             & 5.262       &          \\
rfcn\_resnet101\_coco                                           & Time (sec)    & 2.895/3.151         & 2.951              & 2.946             & 2.872/2.993       & 3.267            & 3.643  & 4.08              & 5.165       &          \\
                                                                & Memory(MiB)   & 10986               & 15610              & 8938              & 8934              & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & OOM                 & 3.606              & 3.595             & 3.623             & 3.892            & 4.154  & 4.705             & 5.514       &          \\
faster\_rcnn\_resnet101\_coco                                   & Time (sec)    & OOM                 & 3.519              & 3.522             & 3.443/3.541       & 3.736            & 4.048  & 4.473             & 5.453       &          \\
                                                                & Memory(MiB)   & 10986               & 15610              & 8938              & 8934              & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & 2.686               & 2.699              & 2.715             & 2.769             & 2.973            & 3.289  & 3.767             & 4.531       &          \\
faster\_rcnn\_resnet101\_lowproposals\_coco                     & Time (sec)    & 2.506               & 2.711              & 2.563             & 2.56              & 2.839            & 3.138  & 3.53              & 4.477       &          \\
                                                                & Memory(MiB)   & 10986               & 15608              & 8938              & 8934              & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & OOM                 & OOM                & OOM               & OOM               & 22.487           & 22.694 & 24.155            & 26.256      &          \\
faster\_rcnn\_inception\_resnet\_v2\_atrous\_coco               & Time (sec)    & OOM                 & OOM                & OOM               & OOM               & 22.365           & 22.524 & 24.003            & 26.32       &          \\
                                                                & Memory(MiB)   &                     &                    &                   &                   & 8938             & 8938   & 8426              & 8170        &          \\
                                                                &               & OOM                 & 7.926              & 7.813             & 7.841             & 8.346            & 8.85   & 10.252            & 12.272      &          \\
faster\_rcnn\_inception\_resnet\_v2\_atrous\_lowproposals\_coco & Time (sec)    & OOM                 & 7.878/8.051        & 7.672/7.866       & 7.782             & 8.258            & 8.745  & 10.09             & 12.048      &          \\
                                                                & Memory(MiB)   &                     &                    &                   &                   & 15608            & 8938   & 8426              & 8170        &          \\
                                                                &               & OOM                 & OOM                & OOM               & OOM               & OOM              & OOM    & 59.964            & 61.885      &          \\
faster\_rcnn\_nas                                               & Time (sec)    & OOM                 & OOM                & OOM               & OOM               & OOM              & OOM    & 59.504            & 61.909      &          \\
                                                                & Memory(MiB)   &                     &                    &                   &                   &                  &        & 8426              & 8170        &          \\
                                                                &               & OOM                 & OOM                & OOM               & 14.261            & 14.721           & 15.792 & 16.589            & 18.475      &          \\
faster\_rcnn\_nas\_lowproposals\_coco                           & Time (sec)    & OOM                 & OOM                & OOM               & 14.202/14.321     & 14.614           & 15.544 & 16.383            & 18.425      &          \\
                                                                & Memory(MiB)   &                     &                    &                   &                   & 9992             & 15610  & 12522             & 8170        &          \\
                                                                &               & OOM                 & OOM                & 16.692            & 16.077            & 16.508           & 17.404 & 18.909            & 21.701      &          \\
mask\_rcnn\_inception\_resnet\_v2\_atrous\_coco                 & Time (sec)    & OOM                 & 16.737             & 16.125            & 16.094/16.125     & 16.389/16.506    & 17.456 & 18.838/18.977     & 21.438      &          \\
                                                                & Memory(MiB)   &                     & 15608              & 8944              & 10992             & 7920             & 8944   & 7920              & 7410        &          \\
                                                                &               & OOM                 & 2.529              & 2.602             & 2.596             & 2.754            & 2.995  & 3.412             & 4.217       &          \\
mask\_rcnn\_inception\_v2\_coco                                 & Time (sec)    & OOM                 & 2.588              & 2.612/2.537       & 2.56              & 2.75             & 2.922  & 3.34              & 4.296       &          \\
                                                                & Memory(MiB)   &                     & 15608              & 8934              & 4838              & 4838             & 3814   & 3302              & 3046        &          \\
                                                                &               & OOM                 & OOM                & 9.792             & 9.427             & 9.567            & 9.932  & 11.635            & 13.683      &          \\
mask\_rcnn\_resnet101\_atrous\_coco                             & Time (sec)    & OOM                 & OOM                & 9.806             & 9.394             & 9.48/9.626       & 10.02  & 11.586            & 13.593      &          \\
                                                                & Memory(MiB)   &                     &                    & 9974              & 8950              & 8950             & 6902   & 5878              & 5366        &          \\
                                                                &               & OOM                 & OOM                & 5.983             & 5.518             & 5.679            & 6.03   & 7.098             & 8.698       &          \\
mask\_rcnn\_resnet50\_atrous\_coco                              & Time (sec)    & OOM                 & OOM                & 5.984             & 5.577             & 5.744            & 6.074  & 7.02/7.158        & 8.645/8.778 &          \\
                                                                & Memory(MiB)   &                     &                    & 9960              & 9448              & 8936             & 6888   & 5866              & 5352        &         
\end{tabular}%
}
\caption{\textbf{Raw Experimental Results.}}
\label{table:raw-results}
\end{table*}

\subsection{Inference Time and Accuracy}
This subsection draws a graph of inference time and accuracy of all models with different batch size.

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{MemoryVSRunning}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:memory-running}
\end{figure}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{MemoryVSRunning-Batch}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:memory-running-batch}
\end{figure}


\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{MemoryVSRunning-Meta}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:memory-running-meta}
\end{figure}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{MemoryVSBatch}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:memory-batch}
\end{figure}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{RunningTimeVSBatch}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:running-batch}
\end{figure}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{RunningTimeVSMemory-Batch}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:running-memory-batch}
\end{figure}

\begin{figure}[htpb]
	  \centering
	  \includegraphics[width=0.5\textwidth]{RunningTimeVSBatch}
	  \caption{\textbf{The correlaration between inference time and memory}}
	  \label{fig:running-batch2}
\end{figure}

\subsection{Memory Consumption and Accuracy}
This subsection draws a graph of memory consumption and accuracy of all models with different batch size.

\subsection{Inference Time and Memory Consumption}
This subsection draws a graph of memory consumption and inference time of all models with different batch size.

\subsection{Inference Time and Accuracy with restricted Memory Consumption}
This subsection draws a graph of largest batch sizes that fit into a limited amount of memory of all models with their inference time and accuracy.

\subsection{Accuracy within Time Restriction}
This subsection draws a graph of maximum accuracy that can be achieved by different time restrictions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~\cite{huang2017speed}~do a similar research, but their experiments are limited to powerful workstations, in which the implementations of the models are developed using Tensorflow framework. The accuracy of this paper's models are evaluated on COCO for object detection.

Benchmarking deep learning system is investigated

\begin{itemize}
    \item MLPerf
    \item DeepBench
    \item Huang~\cite{DBLP:journals/corr/HuangRSZKFFWSG016} investigates the speed/accuracy trade-off f
\end{itemize}

In contrast to the approaches referenced above our benchmark is 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
%\setstretch{1}
%\setlength\bibitemsep{0pt}
\bibliography{deeplearning}




% that's all folks
\end{document}

